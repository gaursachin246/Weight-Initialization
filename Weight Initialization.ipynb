{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0bc53db-423b-40ca-8ec8-4be669add57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer #1: The Importance of Weight Initialization\n",
    "#Weight initialization is essential in artificial neural networks because it sets the stage for how well the model learns. If weights are initialized too high or too low, it can lead to poor learning dynamics. For instance, initializing all weights to zero can cause neurons to learn the same features, effectively making them redundant. On the other hand, initializing weights randomly can help break symmetry and allow the network to learn diverse features.\n",
    "\n",
    "#In summary, careful weight initialization is necessary to ensure that the model can effectively learn from the data, leading to better performance and faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40a58ad2-06ad-427e-a45b-5482cff8a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer #2: Challenges of Improper Weight Initialization\n",
    "#Improper weight initialization can lead to several challenges during model training. One major issue is the vanishing gradient problem, where gradients become too small for the weights to update effectively. This often occurs when weights are initialized too small, causing the network to learn very slowly or get stuck. Conversely, if weights are initialized too large, it can lead to exploding gradients, where the weights update too aggressively, causing instability in training.\n",
    "\n",
    "#These issues can severely affect model convergence, making it difficult for the network to reach an optimal solution. Therefore, understanding and implementing proper weight initialization techniques is crucial for successful training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddfbfa5f-3eee-4faa-96ce-b18db72d87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer #3: The Role of Variance in Weight Initialization\n",
    "#Variance plays a significant role in weight initialization because it directly affects how signals propagate through the network. When initializing weights, it's important to consider the variance to ensure that the outputs of each layer maintain a consistent scale. If the variance is too high or too low, it can lead to the aforementioned vanishing or exploding gradients.\n",
    "\n",
    "#For example, using techniques like He initialization or Xavier initialization helps to set the variance of the weights based on the number of input and output neurons. This careful consideration of variance ensures that the activations remain in a suitable range, promoting effective learning and convergence. Thus, understanding variance is crucial for optimizing weight initialization in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbd790-323a-40bc-87c6-0e8d85cf173c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fa288-0d58-487b-81db-98eaf36c0c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4dc6445-52ac-414c-bd1c-a0d75fa29f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer #1: Zero Initialization\n",
    "#Zero initialization is a straightforward technique where all weights in a neural network are initialized to zero. While this might seem like a simple and effective approach, it comes with significant limitations.\n",
    "\n",
    "#Limitations:\n",
    "#Symmetry Problem: When all weights are initialized to zero, every neuron in a layer learns the same features during training. This symmetry prevents the network from learning effectively, as all neurons will update in the same way.\n",
    "#Stagnation: The gradients during backpropagation will also be the same for all neurons, leading to no updates in weights, effectively stalling the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54dc6bb1-1dc4-4384-a533-9745bb0fc068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer #2: Random Initialization\n",
    "#Random initialization involves setting the weights of a neural network to small random values, typically drawn from a Gaussian or uniform distribution. This technique helps to break the symmetry problem seen in zero initialization.\n",
    "\n",
    "#Adjustments to Mitigate Issues:\n",
    "#Scaling: To prevent saturation in activation functions (like sigmoid or tanh), weights can be scaled based on the number of input and output neurons. For example, using a normal distribution with a mean of 0 and a standard deviation of ( \\sqrt{2/n} ) can help.\n",
    "#Avoiding Vanishing/Exploding Gradients: Techniques like using a uniform distribution within a specific range can help mitigate the risk of gradients becoming too small (vanishing) or too large (exploding) during training.\n",
    "#Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a978b71-3b10-45e9-91fe-4df34f4b080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_initialization(shape):\n",
    "    return np.random.randn(*shape) * np.sqrt(2.0 / (shape[0] + shape[1]))\n",
    "\n",
    "weights = random_initialization((3, 2))  # Example for a layer with 3 inputs and 2 outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13c33127-2679-4bd1-8ece-82d5721d9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_initialization(shape):\n",
    "    limit = np.sqrt(6 / (shape[0] + shape[1]))\n",
    "    return np.random.uniform(-limit, limit, shape)\n",
    "\n",
    "weights = xavier_initialization((3, 2))  # Example for a layer with 3 inputs and 2 outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db77beb1-e91f-44d2-af5a-56fa4078553e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zeros': 0.11349999904632568, 'random_normal': 0.9782999753952026, 'glorot_uniform': 0.9745000004768372, 'he_normal': 0.9761000275611877}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Function to create model with different initializations\n",
    "def create_model(initializer):\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize models with different techniques\n",
    "initializers = ['zeros', 'random_normal', 'glorot_uniform', 'he_normal']\n",
    "results = {}\n",
    "\n",
    "for init in initializers:\n",
    "    model = create_model(init)\n",
    "    model.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    results[init] = test_acc\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae451d-eaa2-4fdd-ada2-82b9a6ee1611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
